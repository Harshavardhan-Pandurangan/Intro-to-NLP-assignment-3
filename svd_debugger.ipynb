{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home2/_harsha_/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home2/_harsha_/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import copy\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "train_data_path = './Data/train.csv'\n",
    "test_data_path = './Data/test.csv'\n",
    "\n",
    "train_data = None\n",
    "test_data = None\n",
    "with open(train_data_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    train_data = list(reader)\n",
    "    train_data = train_data[1:] # remove header\n",
    "with open(test_data_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    test_data = list(reader)\n",
    "    test_data = test_data[1:] # remove header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_description = [row[1] for row in train_data]\n",
    "train_data_tokenised = [nltk.word_tokenize(description) for description in train_data_description]\n",
    "test_data_description = [row[1] for row in test_data]\n",
    "test_data_tokenised = [nltk.word_tokenize(description) for description in test_data_description]\n",
    "\n",
    "train_data_tokenised = [[word.split('\\\\') for word in description] for description in train_data_tokenised]\n",
    "test_data_tokenised = [[word.split('\\\\') for word in description] for description in test_data_tokenised]\n",
    "\n",
    "train_data_temp = copy.deepcopy(train_data_tokenised)\n",
    "test_data_temp = copy.deepcopy(test_data_tokenised)\n",
    "train_data_tokenised = []\n",
    "test_data_tokenised = []\n",
    "for description in train_data_temp:\n",
    "    train_data_tokenised.append([])\n",
    "    for words in description:\n",
    "        for word in words:\n",
    "            if word != '':\n",
    "                train_data_tokenised[-1].append(word)\n",
    "for description in test_data_temp:\n",
    "    test_data_tokenised.append([])\n",
    "    for words in description:\n",
    "        for word in words:\n",
    "            if word != '':\n",
    "                test_data_tokenised[-1].append(word)\n",
    "\n",
    "train_data_tokenised = [[word.lower() for word in description] for description in train_data_tokenised]\n",
    "test_data_tokenised = [[word.lower() for word in description] for description in test_data_tokenised]\n",
    "\n",
    "vocab = list(set([word for description in train_data_tokenised for word in description]))\n",
    "vocab.append('<oov>')\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_index = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_tokenised' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5296/3942417652.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# find the lengths of each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_tokenised\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_data_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_data_tokenised\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# find the length histogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_tokenised' is not defined"
     ]
    }
   ],
   "source": [
    "# find the lengths of each sentence\n",
    "train_data_lengths = [len(description) for description in train_data_tokenised]\n",
    "test_data_lengths = [len(description) for description in test_data_tokenised]\n",
    "\n",
    "# find the length histogram\n",
    "plt.hist(train_data_lengths, bins=100)\n",
    "plt.xlabel('Length of Description')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Description Lengths')\n",
    "plt.show()\n",
    "\n",
    "# find the 90th percentile of the lengths\n",
    "train_data_lengths = np.array(train_data_lengths)\n",
    "train_data_lengths_90 = np.percentile(train_data_lengths, 90)\n",
    "train_data_lengths_95 = np.percentile(train_data_lengths, 95)\n",
    "train_data_lengths_99 = np.percentile(train_data_lengths, 99)\n",
    "print('90th percentile of lengths:', train_data_lengths_90)\n",
    "print('95th percentile of lengths:', train_data_lengths_95)\n",
    "print('99th percentile of lengths:', train_data_lengths_99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/120000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [00:52<00:00, 2279.89it/s]\n"
     ]
    }
   ],
   "source": [
    "co_occurrence_matrix = scipy.sparse.lil_matrix((vocab_size, vocab_size))\n",
    "\n",
    "window_size = 2\n",
    "for description in tqdm.tqdm(train_data_tokenised):\n",
    "    for i in range(len(description)):\n",
    "        for j in range(i+1, min(i+window_size, len(description))):\n",
    "            a,b = word_index[description[i]], word_index[description[j]]\n",
    "            co_occurrence_matrix[a,b] += 1\n",
    "            co_occurrence_matrix[b,a] += 1\n",
    "\n",
    "co_occurrence_matrix = co_occurrence_matrix.tocsr()\n",
    "\n",
    "u, s, vt = scipy.sparse.linalg.svds(co_occurrence_matrix.astype(np.float64), k=50)\n",
    "\n",
    "embeddings = u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/_harsha_/miniconda3/envs/gaussian_splatting/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484801627/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_data_tokenised)):\n",
    "    for j in range(len(test_data_tokenised[i])):\n",
    "        if test_data_tokenised[i][j] not in word_index:\n",
    "            test_data_tokenised[i][j] = '<oov>'\n",
    "\n",
    "train_data_x = []\n",
    "for description in train_data_tokenised:\n",
    "    train_data_x.append([embeddings[word_index[word]] for word in description])\n",
    "test_data_x = []\n",
    "for description in test_data_tokenised:\n",
    "    test_data_x.append([embeddings[word_index[word]] for word in description])\n",
    "\n",
    "train_data_x = nn.utils.rnn.pad_sequence([torch.tensor(description) for description in train_data_x], batch_first=True)\n",
    "test_data_x = nn.utils.rnn.pad_sequence([torch.tensor(description) for description in test_data_x], batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120000, 245, 50])\n",
      "torch.Size([7600, 160, 50])\n"
     ]
    }
   ],
   "source": [
    "print(train_data_x.shape)\n",
    "print(test_data_x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaussian_splatting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
