{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INLP Assignment 3\n",
    "### Harshavardhan P - 2021111003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/harshavardhan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/harshavardhan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import copy\n",
    "import scipy\n",
    "import numpy as np\n",
    "import sklearn.decomposition\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data from csv files\n",
    "train_data_path = './Data/train.csv'\n",
    "test_data_path = './Data/test.csv'\n",
    "\n",
    "train_data = None\n",
    "test_data = None\n",
    "with open(train_data_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    train_data = list(reader)\n",
    "    train_data = train_data[1:] # remove header\n",
    "with open(test_data_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    test_data = list(reader)\n",
    "    test_data = test_data[1:] # remove header\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenising the train data to get the individual words\n",
    "train_data_description = [row[1] for row in train_data]\n",
    "train_data_tokenised = [nltk.word_tokenize(description) for description in train_data_description]\n",
    "# tokenise the words based on '\\\\' as well\n",
    "train_data_tokenised = [[word.split('\\\\') for word in description] for description in train_data_tokenised]\n",
    "\n",
    "# print all the elements in the list within the tokenised list\n",
    "train_data_temp = copy.deepcopy(train_data_tokenised)\n",
    "train_data_tokenised = []\n",
    "for description in train_data_temp:\n",
    "    train_data_tokenised.append([])\n",
    "    for words in description:\n",
    "        for word in words:\n",
    "            if word != '':\n",
    "                train_data_tokenised[-1].append(word)\n",
    "\n",
    "# convert all the words to lower case\n",
    "train_data_tokenised = [[word.lower() for word in description] for description in train_data_tokenised]\n",
    "\n",
    "print(len(train_data_tokenised))\n",
    "print(train_data_tokenised[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the co-occurance matrix for the words\n",
    "vocab = list(set([word for description in train_data_tokenised for word in description]))\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "# make a dictionary for the words and their index\n",
    "word_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# co_occurance_matrix = numpy.zeros((vocab_size, vocab_size))\n",
    "\n",
    "# for description in tqdm.tqdm(train_data_tokenised):\n",
    "#     for i in range(len(description)):\n",
    "#         for j in range(i+1, len(description)):\n",
    "#             co_occurance_matrix[word_index[description[i]], word_index[description[j]]] += 1\n",
    "#             co_occurance_matrix[word_index[description[j]], word_index[description[i]]] += 1\n",
    "\n",
    "# print(co_occurance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # svd on the co-occurance matrix (limited computation)\n",
    "# # import scipy.sparse.linalg\n",
    "# # u, s, vt = scipy.sparse.linalg.svds(co_occurance_matrix, k=100)\n",
    "\n",
    "# import sklearn.decomposition\n",
    "# svd = sklearn.decomposition.TruncatedSVD(n_components=2)\n",
    "# u = svd.fit_transform(co_occurance_matrix)\n",
    "# s = svd.singular_values_\n",
    "# vt = svd.components_\n",
    "\n",
    "# print(u.shape)\n",
    "# print(s.shape)\n",
    "# print(vt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the co-occurance matrix for the words\n",
    "co_occurance_matrix = scipy.sparse.lil_matrix((vocab_size, vocab_size))\n",
    "\n",
    "# make the co-occurance matrix using the window size of 2\n",
    "window_size = 2\n",
    "for description in tqdm.tqdm(train_data_tokenised):\n",
    "    for i in range(len(description)):\n",
    "        for j in range(i+1, min(i+window_size, len(description))):\n",
    "            a,b = word_index[description[i]], word_index[description[j]]\n",
    "            co_occurance_matrix[a,b] += 1\n",
    "            co_occurance_matrix[b,a] += 1\n",
    "\n",
    "co_occurance_matrix = co_occurance_matrix.tocsr()\n",
    "\n",
    "print(co_occurance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svd on the co-occurance matrix (limited computation)\n",
    "u, s, vt = scipy.sparse.linalg.svds(co_occurance_matrix.astype(np.float64), k=500)\n",
    "\n",
    "print(u.shape)\n",
    "print(s.shape)\n",
    "print(vt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating embeddings by Singular Value Decomposition\n",
    "def gen_svd_embeddings(data, window_size=2):\n",
    "    # tokenising the train data to get the individual words\n",
    "    train_data_description = [row[1] for row in data]\n",
    "    train_data_tokenised = [nltk.word_tokenize(description) for description in train_data_description]\n",
    "    # tokenise the words based on '\\\\' as well\n",
    "    train_data_tokenised = [[word.split('\\\\') for word in description] for description in train_data_tokenised]\n",
    "\n",
    "    # print all the elements in the list within the tokenised list\n",
    "    train_data_temp = copy.deepcopy(train_data_tokenised)\n",
    "    train_data_tokenised = []\n",
    "    for description in train_data_temp:\n",
    "        train_data_tokenised.append([])\n",
    "        for words in description:\n",
    "            for word in words:\n",
    "                if word != '':\n",
    "                    train_data_tokenised[-1].append(word)\n",
    "\n",
    "    # convert all the words to lower case\n",
    "    train_data_tokenised = [[word.lower() for word in description] for description in train_data_tokenised]\n",
    "\n",
    "    # making the co-occurance matrix for the words\n",
    "    vocab = list(set([word for description in train_data_tokenised for word in description]))\n",
    "    vocab = sorted(vocab)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # make a dictionary for the words and their index\n",
    "    word_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    # making the co-occurance matrix for the words\n",
    "    co_occurance_matrix = scipy.sparse.lil_matrix((vocab_size, vocab_size))\n",
    "\n",
    "    # make the co-occurance matrix using the window size of 2\n",
    "    for description in tqdm.tqdm(train_data_tokenised):\n",
    "        for i in range(len(description)):\n",
    "            for j in range(i+1, min(i+window_size, len(description))):\n",
    "                a,b = word_index[description[i]], word_index[description[j]]\n",
    "                co_occurance_matrix[a,b] += 1\n",
    "                co_occurance_matrix[b,a] += 1\n",
    "\n",
    "    co_occurance_matrix = co_occurance_matrix.tocsr()\n",
    "\n",
    "    # svd on the co-occurance matrix (limited computation)\n",
    "    u, s, vt = scipy.sparse.linalg.svds(co_occurance_matrix.astype(np.float64), k=500)\n",
    "\n",
    "    return u, vocab, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the dataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the RNN classifier with torch\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Classifier class\n",
    "class RNNTrainer:\n",
    "    # defining the NewsDataset class\n",
    "    class NewsDataset(Dataset):\n",
    "        def __init__(self, embeddings, labels):\n",
    "            self.embeddings = embeddings\n",
    "            self.labels = labels\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.embeddings)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "    # defining the RNN classifier with torch\n",
    "    class RNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super(self.RNN, self).__init__()\n",
    "\n",
    "            self.hidden_size = hidden_size\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "            self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "            out, _ = self.rnn(x, h0)\n",
    "            out = self.fc(out)\n",
    "            out = self.softmax(out)\n",
    "            return out\n",
    "\n",
    "    # defining the constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data=None,\n",
    "        test_data=None,\n",
    "        batch_size=32,\n",
    "        embedding_dim=500,\n",
    "        embedding_type='svd',\n",
    "        svd_context_size=2,\n",
    "        hidden_size=128,\n",
    "        epochs=10,\n",
    "        save_model=False,\n",
    "        load_model=False,\n",
    "        model_path=None):\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_type = embedding_type\n",
    "        self.svd_context_size = svd_context_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.epochs = epochs\n",
    "        self.save_model = save_model\n",
    "        self.load_model = load_model\n",
    "        self.model_path = model_path\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.prepare_data()\n",
    "        self.create_model()\n",
    "        self.train()\n",
    "        self.test()\n",
    "        self.plot_train_loss_acc()\n",
    "\n",
    "    # generating svd embeddings\n",
    "    def gen_svd_embeddings(self):\n",
    "        # making the co-occurance matrix for the words\n",
    "        co_occurance_matrix = scipy.sparse.lil_matrix((self.vocab_size, self.vocab_size))\n",
    "\n",
    "        # make the co-occurance matrix using the window size of 2\n",
    "        window_size = self.svd_context_size\n",
    "        for description in tqdm.tqdm(self.train_data_tokenised):\n",
    "            for i in range(len(description)):\n",
    "                for j in range(i+1, min(i+window_size, len(description))):\n",
    "                    a,b = self.word_index[description[i]], self.word_index[description[j]]\n",
    "                    co_occurance_matrix[a,b] += 1\n",
    "                    co_occurance_matrix[b,a] += 1\n",
    "\n",
    "        co_occurance_matrix = co_occurance_matrix.tocsr()\n",
    "\n",
    "        # svd on the co-occurance matrix (limited computation)\n",
    "        u, s, vt = scipy.sparse.linalg.svds(co_occurance_matrix.astype(np.float64), k=self.embedding_dim)\n",
    "\n",
    "        self.embeddings = u\n",
    "\n",
    "    # generating word2vec embeddings\n",
    "    def gen_word2vec_embeddings(self):\n",
    "        pass\n",
    "\n",
    "    # function to generate embeddings and initialise the dataset and dataloader\n",
    "    def prepare_data(self):\n",
    "        # tokenising the train data to get the individual words\n",
    "        train_data_description = [row[1] for row in self.train_data]\n",
    "        train_data_tokenised = [nltk.word_tokenize(description) for description in train_data_description]\n",
    "        # tokenise the words based on '\\\\' as well\n",
    "        train_data_tokenised = [[word.split('\\\\') for word in description] for description in train_data_tokenised]\n",
    "\n",
    "        # print all the elements in the list within the tokenised list\n",
    "        train_data_temp = copy.deepcopy(train_data_tokenised)\n",
    "        train_data_tokenised = []\n",
    "        for description in train_data_temp:\n",
    "            train_data_tokenised.append([])\n",
    "            for words in description:\n",
    "                for word in words:\n",
    "                    if word != '':\n",
    "                        train_data_tokenised[-1].append(word)\n",
    "\n",
    "        # convert all the words to lower case\n",
    "        self.train_data_tokenised = [[word.lower() for word in description] for description in train_data_tokenised]\n",
    "\n",
    "        # making the co-occurance matrix for the words\n",
    "        vocab = list(set([word for description in self.train_data_tokenised for word in description]))\n",
    "        # include the <oov> token to the vocab\n",
    "        vocab.append('<oov>')\n",
    "        self.vocab = sorted(vocab)\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "        # make a dictionary for the words and their index\n",
    "        self.word_index = {word: i for i, word in enumerate(self.vocab)}\n",
    "\n",
    "        if self.embedding_type == 'svd':\n",
    "            self.gen_svd_embeddings()\n",
    "        elif self.embedding_type == 'word2vec':\n",
    "            self.gen_word2vec_embeddings()\n",
    "        else:\n",
    "            print('Invalid embedding type')\n",
    "            return\n",
    "\n",
    "        self.train_data_x = [[] for _ in range(len(self.train_data))]\n",
    "        self.test_data_x = [[] for _ in range(len(self.test_data))]\n",
    "        for i in range(len(self.train_data_tokenised)):\n",
    "            for j in range(len(self.train_data_tokenised[i])):\n",
    "                self.train_data_x[i].append(self.embeddings[self.word_index[self.train_data_tokenised[i][j]]])\n",
    "        for i in range(len(self.test_data)):\n",
    "            for j in range(len(self.train_data_tokenised[i])):\n",
    "                self.test_data_x[i].append(self.embeddings[self.word_index[self.train_data_tokenised[i][j]]])\n",
    "\n",
    "        train_labels = [row[0] for row in self.train_data]\n",
    "\n",
    "        # one hot encoding the labels\n",
    "        labels = list(set(train_labels))\n",
    "        label_index = {label: i for i, label in enumerate(labels)}\n",
    "        labels_onehot_train = np.zeros((len(self.train_data), len(labels)))\n",
    "        labels_onehot_test = np.zeros((len(self.test_data), len(labels)))\n",
    "        for i in range(len(self.train_data)):\n",
    "            labels_onehot_train[i, label_index[self.train_data[i][0]]] = 1\n",
    "        for i in range(len(self.test_data)):\n",
    "            labels_onehot_test[i, label_index[self.test_data[i][0]]] = 1\n",
    "\n",
    "        self.train_data_y = labels_onehot_train\n",
    "        self.test_data_y = labels_onehot_test\n",
    "\n",
    "        self.train_dataset = self.NewsDataset(self.train_data_x, self.train_data_y)\n",
    "        self.test_dataset = self.NewsDataset(self.test_data_x, self.test_data_y)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = self.RNN(input_size=self.embedding_dim, hidden_size=self.hidden_size, output_size=self.train_data_y.shape[1]).to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        for epoch in tqdm.tqdm(range(self.epochs)):\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for i, data in enumerate(self.train_loader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, torch.argmax(labels, dim=1))\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                total += labels.size(0)\n",
    "                correct += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "            self.train_loss.append(running_loss)\n",
    "            self.train_accuracy.append(correct/total)\n",
    "\n",
    "        print('Training Loss: ', self.train_loss[-1])\n",
    "        print('Training Accuracy: ', self.train_accuracy[-1])\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in self.test_loader:\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                total += labels.size(0)\n",
    "                correct += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "        self.test_accuracy = correct/total\n",
    "        print('Test Accuracy: ', self.test_accuracy)\n",
    "\n",
    "    def plot_train_loss_acc(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.train_loss, label='Train Loss')\n",
    "        plt.plot(self.train_accuracy, label='Train Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self):\n",
    "        pass\n",
    "\n",
    "    def load_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = 'Hi there how are you doing today'\n",
    "sentence2 = 'Hi I am doing good today'\n",
    "sentence3 = 'Are you doing good today'\n",
    "\n",
    "# tokenising the train data to get the individual words\n",
    "train_data_description = [sentence1, sentence2, sentence3]\n",
    "train_data_tokenised = [nltk.word_tokenize(description) for description in train_data_description]\n",
    "# tokenise the words based on '\\\\' as well\n",
    "train_data_tokenised = [[word.split('\\\\') for word in description] for description in train_data_tokenised]\n",
    "\n",
    "# print all the elements in the list within the tokenised list\n",
    "train_data_temp = copy.deepcopy(train_data_tokenised)\n",
    "train_data_tokenised = []\n",
    "for description in train_data_temp:\n",
    "    train_data_tokenised.append([])\n",
    "    for words in description:\n",
    "        for word in words:\n",
    "            if word != '':\n",
    "                train_data_tokenised[-1].append(word)\n",
    "\n",
    "# convert all the words to lower case\n",
    "train_data_tokenised = [[word.lower() for word in description] for description in train_data_tokenised]\n",
    "\n",
    "vocab = list(set([word for description in train_data_tokenised for word in description]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(vocab_size)\n",
    "print(train_data_tokenised)\n",
    "\n",
    "word_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "print(word_index)\n",
    "\n",
    "# making the co-occurance matrix for the words\n",
    "co_occurance_matrix = scipy.sparse.lil_matrix((vocab_size, vocab_size))\n",
    "window_size = 2\n",
    "for description in tqdm.tqdm(train_data_tokenised):\n",
    "    for i in range(len(description)):\n",
    "        for j in range(i+1, min(i+window_size, len(description))):\n",
    "            a,b = word_index[description[i]], word_index[description[j]]\n",
    "            print(description[i], description[j], a, b)\n",
    "            co_occurance_matrix[a,b] += 1\n",
    "            co_occurance_matrix[b,a] += 1\n",
    "\n",
    "co_occurance_matrix = co_occurance_matrix.tocsr()\n",
    "\n",
    "print(co_occurance_matrix.shape)\n",
    "print(co_occurance_matrix.astype(np.float64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
