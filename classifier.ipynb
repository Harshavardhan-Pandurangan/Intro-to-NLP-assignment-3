{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import copy\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the NewsDataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, embeddings, pad_ids, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.pad_ids = pad_ids\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.pad_ids[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Classifier class\n",
    "class RNNTrainer:\n",
    "    # defining the RNN classifier with torch\n",
    "    class RNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size):\n",
    "            super().__init__()\n",
    "\n",
    "            self.hidden_size = hidden_size\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "            self.fc1 = nn.Linear(hidden_size, hidden_size//2)\n",
    "            self.fc2 = nn.Linear(hidden_size//2, output_size)\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "        def forward(self, x, ids):\n",
    "            out, _ = self.rnn(x)\n",
    "            out = self.fc1(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.fc2(out)\n",
    "            return out\n",
    "\n",
    "    # defining the constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data=None,\n",
    "        test_data=None,\n",
    "        batch_size=1,\n",
    "        embedding_dim=100,\n",
    "        embedding_type='svd',\n",
    "        svd_context_size=2,\n",
    "        hidden_size=128,\n",
    "        epochs=10,\n",
    "        save_model=False,\n",
    "        load_model=False,\n",
    "        model_path=None):\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_type = embedding_type\n",
    "        self.svd_context_size = svd_context_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.epochs = epochs\n",
    "        self.save_model = save_model\n",
    "        self.load_model = load_model\n",
    "        self.dataset = NewsDataset\n",
    "        self.model_path = model_path\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.train_losses = []\n",
    "\n",
    "        self.prepare_data()\n",
    "        self.create_model()\n",
    "        for epoch in range(self.epochs):\n",
    "            print('Epoch: ', epoch+1)\n",
    "            self.train()\n",
    "            self.test()\n",
    "        self.plot_train_loss_acc()\n",
    "\n",
    "    # generating svd embeddings\n",
    "    def gen_svd_embeddings(self):\n",
    "        # making the co-occurance matrix for the words\n",
    "        co_occurance_matrix = scipy.sparse.lil_matrix((self.vocab_size, self.vocab_size))\n",
    "\n",
    "        # make the co-occurance matrix using the window size of 2\n",
    "        window_size = self.svd_context_size\n",
    "        for description in tqdm.tqdm(self.train_data_tokenised):\n",
    "            for i in range(len(description)):\n",
    "                for j in range(i+1, min(i+window_size, len(description))):\n",
    "                    a,b = self.word_index[description[i]], self.word_index[description[j]]\n",
    "                    co_occurance_matrix[a,b] += 1\n",
    "                    co_occurance_matrix[b,a] += 1\n",
    "\n",
    "        co_occurance_matrix = co_occurance_matrix.tocsr()\n",
    "\n",
    "        # svd on the co-occurance matrix (limited computation)\n",
    "        u, s, vt = scipy.sparse.linalg.svds(co_occurance_matrix.astype(np.float64), k=self.embedding_dim)\n",
    "        del co_occurance_matrix\n",
    "\n",
    "        # make sure the most important dimensions are first\n",
    "        s = s[::-1]\n",
    "        u = u[:, ::-1]\n",
    "\n",
    "        self.embeddings = u @ np.diag(np.sqrt(s))\n",
    "\n",
    "    # generating word2vec embeddings\n",
    "    def gen_word2vec_embeddings(self):\n",
    "        pass\n",
    "\n",
    "    # function to generate embeddings and initialise the dataset and dataloader\n",
    "    def prepare_data(self):\n",
    "        # tokenising the train data to get the individual words\n",
    "        train_data_description = [row[1] for row in self.train_data]\n",
    "        train_data_tokenised = [nltk.word_tokenize(description) for description in train_data_description]\n",
    "        test_data_description = [row[1] for row in self.test_data]\n",
    "        test_data_tokenised = [nltk.word_tokenize(description) for description in test_data_description]\n",
    "        # tokenise the words based on '\\\\' as well\n",
    "        train_data_tokenised = [[word.split('\\\\') for word in description] for description in train_data_tokenised]\n",
    "        test_data_tokenised = [[word.split('\\\\') for word in description] for description in test_data_tokenised]\n",
    "\n",
    "        # print all the elements in the list within the tokenised list\n",
    "        train_data_temp = copy.deepcopy(train_data_tokenised)\n",
    "        test_data_temp = copy.deepcopy(test_data_tokenised)\n",
    "        train_data_tokenised = []\n",
    "        test_data_tokenised = []\n",
    "        for description in train_data_temp:\n",
    "            train_data_tokenised.append([])\n",
    "            for words in description:\n",
    "                for word in words:\n",
    "                    if word != '':\n",
    "                        train_data_tokenised[-1].append(word)\n",
    "        for description in test_data_temp:\n",
    "            test_data_tokenised.append([])\n",
    "            for words in description:\n",
    "                for word in words:\n",
    "                    if word != '':\n",
    "                        test_data_tokenised[-1].append(word)\n",
    "\n",
    "        # convert all the words to lower case\n",
    "        self.train_data_tokenised = [[word.lower() for word in description] for description in train_data_tokenised]\n",
    "        self.test_data_tokenised = [[word.lower() for word in description] for description in test_data_tokenised]\n",
    "\n",
    "        # making the co-occurance matrix for the words\n",
    "        vocab = list(set([word for description in self.train_data_tokenised for word in description]))\n",
    "        # include the <oov> token to the vocab\n",
    "        vocab.append('<oov>')\n",
    "        self.vocab = sorted(vocab)\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "        # make a dictionary for the words and their index\n",
    "        self.word_index = {word: i for i, word in enumerate(self.vocab)}\n",
    "\n",
    "        if self.embedding_type == 'svd':\n",
    "            self.gen_svd_embeddings()\n",
    "        elif self.embedding_type == 'word2vec':\n",
    "            self.gen_word2vec_embeddings()\n",
    "        else:\n",
    "            print('Invalid embedding type')\n",
    "            return\n",
    "\n",
    "        # replace all the words not in the vocab with <oov>\n",
    "        for i in range(len(self.test_data_tokenised)):\n",
    "            for j in range(len(self.test_data_tokenised[i])):\n",
    "                if self.test_data_tokenised[i][j] not in self.word_index:\n",
    "                    self.test_data_tokenised[i][j] = '<oov>'\n",
    "\n",
    "        # convert the tokenised data to embeddings\n",
    "        self.train_data_x = []\n",
    "        for description in self.train_data_tokenised:\n",
    "            embeddings = [self.embeddings[self.word_index[word]] for word in description]\n",
    "            self.train_data_x.append(embeddings)\n",
    "        self.test_data_x = []\n",
    "        for description in self.test_data_tokenised:\n",
    "            embeddings = [self.embeddings[self.word_index[word]] for word in description]\n",
    "            self.test_data_x.append(embeddings)\n",
    "\n",
    "        # find the lengths of all the sequences\n",
    "        self.train_data_x_lens = [len(description) for description in self.train_data_tokenised]\n",
    "        self.test_data_x_lens = [len(description) for description in self.test_data_tokenised]\n",
    "        # find the 95th percentile of the lengths\n",
    "        self.train_data_x_len = np.percentile(self.train_data_x_lens, 90)\n",
    "        self.test_data_x_len = np.percentile(self.test_data_x_lens, 90)\n",
    "        # pad the sequence\n",
    "        self.train_data_x = nn.utils.rnn.pad_sequence([torch.tensor(np.array(embeddings), dtype=torch.float32) for embeddings in self.train_data_x], batch_first=True, padding_value=0)\n",
    "        self.test_data_x = nn.utils.rnn.pad_sequence([torch.tensor(np.array(embeddings), dtype=torch.float32) for embeddings in self.test_data_x], batch_first=True, padding_value=0)\n",
    "        # trim the sequences to the 90th percentile length\n",
    "        self.train_data_x = self.train_data_x[:, :int(self.train_data_x_len)]\n",
    "        self.test_data_x = self.test_data_x[:, :int(self.test_data_x_len)]\n",
    "        # reduce lens to the truncated length\n",
    "        self.train_data_x_lens = [min(self.train_data_x_lens[i], int(self.train_data_x_len)) for i in range(len(self.train_data_x_lens))]\n",
    "        self.test_data_x_lens = [min(self.test_data_x_lens[i], int(self.test_data_x_len)) for i in range(len(self.test_data_x_lens))]\n",
    "\n",
    "        train_labels = [row[0] for row in self.train_data]\n",
    "\n",
    "        # one hot encoding the labels\n",
    "        labels = list(set(train_labels))\n",
    "        label_index = {label: i for i, label in enumerate(labels)}\n",
    "        labels_onehot_train = np.zeros((len(self.train_data), len(labels)))\n",
    "        labels_onehot_test = np.zeros((len(self.test_data), len(labels)))\n",
    "        for i in range(len(self.train_data)):\n",
    "            labels_onehot_train[i, label_index[self.train_data[i][0]]] = 1\n",
    "        for i in range(len(self.test_data)):\n",
    "            labels_onehot_test[i, label_index[self.test_data[i][0]]] = 1\n",
    "\n",
    "        self.train_data_y = labels_onehot_train\n",
    "        self.test_data_y = labels_onehot_test\n",
    "\n",
    "        self.train_dataset = NewsDataset(self.train_data_x, self.train_data_x_lens, self.train_data_y)\n",
    "        self.test_dataset = NewsDataset(self.test_data_x, self.test_data_x_lens, self.test_data_y)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    def create_model(self):\n",
    "        self.model = self.RNN(input_size=self.embedding_dim, hidden_size=self.hidden_size, output_size=self.train_data_y.shape[1]).to(self.device)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.train_loss = []\n",
    "        self.train_accuracy = []\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data in enumerate(tqdm.tqdm(self.train_loader)):\n",
    "            inputs, ids, labels = data\n",
    "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs, ids)\n",
    "            # sum the outputs\n",
    "            outputs = torch.sum(outputs, dim=1)\n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "            self.train_losses.append(loss.item())\n",
    "\n",
    "        self.train_loss.append(running_loss)\n",
    "        self.train_accuracy.append(correct/total)\n",
    "\n",
    "        print('Training Loss: ', self.train_loss[-1])\n",
    "        print('Training Accuracy: ', self.train_accuracy[-1])\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in self.test_loader:\n",
    "                inputs, ids, labels = data\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = self.model(inputs, ids)\n",
    "                outputs = torch.sum(outputs, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "        print('Test Accuracy: ', correct/total)\n",
    "\n",
    "    def plot_train_loss_acc(self):\n",
    "        plt.figure()\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Train Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def save_model(self):\n",
    "        pass\n",
    "\n",
    "    def load_model(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function\n",
    "if __name__ == \"__main__\":\n",
    "    # load the data\n",
    "    train_data_path = './Data/train.csv'\n",
    "    test_data_path = './Data/test.csv'\n",
    "\n",
    "    train_data = None\n",
    "    test_data = None\n",
    "    with open(train_data_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        train_data = list(reader)\n",
    "        train_data = train_data[1:] # remove header\n",
    "    with open(test_data_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        test_data = list(reader)\n",
    "        test_data = test_data[1:] # remove header\n",
    "\n",
    "    params = {\n",
    "        'train_data':train_data,\n",
    "        'test_data':test_data,\n",
    "        'batch_size':128,\n",
    "        'embedding_dim':100,\n",
    "        'embedding_type':'svd',\n",
    "        'svd_context_size':2,\n",
    "        'hidden_size':256\n",
    "    }\n",
    "\n",
    "    rnn_trainer = RNNTrainer(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the train losses\n",
    "plt.figure()\n",
    "plt.plot(rnn_trainer.train_losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
